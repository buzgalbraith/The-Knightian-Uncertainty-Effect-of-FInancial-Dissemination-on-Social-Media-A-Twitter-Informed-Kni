{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2da3fcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import re\n",
    "import pandas as pd \n",
    "import chardet\n",
    "import numpy as np\n",
    "import os\n",
    "from pprint import pprint\n",
    "#from google.cloud import storage\n",
    "#from google.cloud import language_v1\n",
    "from dateutil import parser\n",
    "from datetime import datetime\n",
    "import matplotlib as plt\n",
    "import yfinance as yf\n",
    "from sklearn.metrics import mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7b5788f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## functions to keep code easily modular. \n",
    "\n",
    "# loads the intial J_Son file \n",
    "def get_file(File):\n",
    "    enc=chardet.detect(open(File,'rb').read())['encoding']\n",
    "    with open(File,'r', encoding = enc) as f:\n",
    "        json_file=json.load(f)\n",
    "        f.close()\n",
    "    return json_file\n",
    "\n",
    "# this formats the json files data as a data frame it requites the users name. \n",
    "def get_data_frame(json_file, name): # re-write this to save as data frame and \n",
    "    texts=[]\n",
    "    like_count=[]\n",
    "    quote_count=[]\n",
    "    reply_count=[]\n",
    "    retweet_count=[]\n",
    "    created_at=[]\n",
    "    id=[]\n",
    "    for i in range(len(json_file[\"data\"])):\n",
    "        texts.append(json_file[\"data\"][i][\"text\"].lower())\n",
    "        created_at.append(json_file[\"data\"][i][\"created_at\"])\n",
    "        retweet_count.append(json_file[\"data\"][i]['public_metrics']['retweet_count'])\n",
    "        reply_count.append(json_file[\"data\"][i]['public_metrics']['reply_count'])\n",
    "        like_count.append(json_file[\"data\"][i]['public_metrics']['like_count'])\n",
    "        quote_count.append(json_file[\"data\"][i]['public_metrics']['quote_count'])\n",
    "        id.append(json_file[\"data\"][i]['id'])\n",
    "    data_frame=pd.DataFrame()\n",
    "    data_frame[\"User Name\"]=[name]*len(texts)\n",
    "    data_frame[\"orignal texts\"]=texts\n",
    "    data_frame[\"created at\"]= created_at\n",
    "    data_frame[\"retweet count\"]=retweet_count\n",
    "    data_frame[\"reply count\"]=reply_count\n",
    "    data_frame[\"like count\"]=like_count\n",
    "    data_frame[\"quote count\"]=quote_count\n",
    "    data_frame[\"id\"]=id\n",
    "    return data_frame\n",
    "\n",
    "def data_for_period(symbol):\n",
    "    # takes a stock symbol name as input and outputs and saves csv over the period of analysis for this paper.\n",
    "    ticker_object=yf.Ticker(symbol)\n",
    "    historical = ticker_object.history(start=\"2021-08-01\", end=\"2021-12-01\", interval=\"1d\")\n",
    "    historical.to_csv(symbol+\".csv\")\n",
    "\n",
    "\n",
    "def standardized_sum(df, col_names):\n",
    "    #takes data frame and a list of colomn names sums and standrdizes those coloumsn \n",
    "    df[\"temp\"]=df[col_names].sum(axis=1) \n",
    "    df[\"Sum of \"+str(col_names)+\"Standardized\"]=(df[\"temp\"]-np.mean(df[\"temp\"]))/np.std(df[\"temp\"])\n",
    "    df=df.drop('temp',axis=1)\n",
    "    return df\n",
    "\n",
    "# this takes a data frame as well as a pattern and only finds the tweets mathcing that pattenr\n",
    "def scrape(pattern, data_frame):\n",
    "    clean_df=data_frame\n",
    "    for i in range(len(data_frame[\"orignal texts\"])):\n",
    "        a=re.search(pattern,data_frame[\"orignal texts\"][i])\n",
    "       # print(a)\n",
    "        if(a==None):\n",
    "            data_frame=data_frame.drop(i,axis=0)\n",
    "            clean_df=clean_text(data_frame)\n",
    "    return clean_df\n",
    "\n",
    "# this will make a pattern with a list of our terms outside of the snp 500 from a csv\n",
    "def get_terms():\n",
    "    raw_terms=pd.read_excel(r\"C:\\Users\\buzga\\Desktop\\Coding Stuff\\Twitter Reaserch\\Data\\Pattern\\TermsExcel.xlsx\")\n",
    "    pattern_2=\"\"\n",
    "    for i in range(len(raw_terms[\"terms\"])-1):\n",
    "        pattern_2=pattern_2+\"^\"+raw_terms[\"terms\"][i].lower()+\"\\W|\\W\"+raw_terms[\"terms\"][i].lower()+\"\\W|\"\n",
    "    pattern_2=pattern_2+\"^\"+raw_terms[\"terms\"][i+1].lower()+\"\\W|\\W\"+raw_terms[\"terms\"][i+1].lower()+\"\\W\"\n",
    "    return pattern_2\n",
    "# this creates the pattern for the scrape method \n",
    "def get_pattern(path):\n",
    "    # for me it is \"C:\\Users\\buzga\\Desktop\\Coding Stuff\\Twitter Reaserch\\constituents_csv.csv\"\n",
    "    # either way here is a link to the download: https://datahub.io/core/s-and-p-500-companies#data\n",
    "    companies=pd.read_csv(path)\n",
    "    companies=companies.drop(\"Sector\", axis=1)\n",
    "    # this list is in alphabetical order starting with the company 3m so we skip the first as it would make s add another case. \n",
    "    pattern=\" \"\n",
    "    for i in range(len(companies[\"Name\"])-1):\n",
    "        pattern=pattern+\"^\"+companies[\"Name\"][i].lower()+ \"\\W|\\W\"+companies[\"Name\"][i].lower()+\"\\W|\"\n",
    "        pattern=pattern+\"^\"+companies[\"Symbol\"][i].lower()+ \"\\W|\\W\"+companies[\"Symbol\"][i].lower()+\"\\W|\"\n",
    "    pattern=pattern+\"^\"+companies[\"Name\"][len(companies[\"Name\"])-1].lower()+ \"\\W|\\W\"+companies[\"Name\"][len(companies[\"Name\"])-1].lower()+\"\\W|\"\n",
    "    pattern=pattern+\"^\"+companies[\"Symbol\"][len(companies[\"Name\"])-1].lower()+ \"\\W|\\W\"+companies[\"Symbol\"][len(companies[\"Name\"])-1].lower()+\"\\W\"\n",
    "    return pattern\n",
    "\n",
    "# this adds a specfic list of words to the SnP500 pattern. \n",
    "def new_pattern(path, words):\n",
    "    pattern=get_pattern(path)\n",
    "    for i in range(len(words)-1):\n",
    "        pattern=pattern+\"^\"+words[i]+\"\\W|\\W\"+words[i]+\"\\W|\"\n",
    "    pattern=pattern+\"^\"+words[len(words)-1]+\"\\W|\\W\"+words[len(words)-1]+\"\\W|\"\n",
    "\n",
    "\n",
    "# this will import a lexicon \n",
    "\n",
    "def get_lexicon(lexicon_path):\n",
    "    names=[\"token\", \"polarity\", \"standerd deviation\", \"raw scores\"]\n",
    "    lexicon=pd.read_csv(lexicon_path, sep=\"\\t\" , names=(names))\n",
    "    lexicon=lexicon.set_index(\"token\")\n",
    "    lexicon.drop(['standerd deviation',\"raw scores\"], axis=1, inplace=True)\n",
    "    return lexicon\n",
    "\n",
    "\n",
    "# this is just a formating method \n",
    "def clean_text(data_frame):\n",
    "    PuncuationPattern = r'[^\\w\\s]|[Ã¢|_]'\n",
    "    WhiteSpacePattern=r'\\s{2,}'\n",
    "    no_punc=[]\n",
    "    for text in data_frame[\"orignal texts\"]:\n",
    "        f=re.sub(PuncuationPattern, \" \", text)\n",
    "        g=re.sub(WhiteSpacePattern, \"\", f)\n",
    "        no_punc.append(f) \n",
    "    if(len(data_frame)!=0):\n",
    "        data_frame['cleaned texts'] = no_punc\n",
    "    else:\n",
    "        data_frame['cleaned texts'] = \"\"\n",
    "    return data_frame\n",
    "\n",
    "# this will add a coloumn to teh previous data frame called polarity represetning sentminet \n",
    "def Lexicon_analyze_sentiment(data_frame,lexicon):\n",
    "    if(\"cleaned texts\" in data_frame.columns and len(data_frame)!=0 ):\n",
    "        data_frame=data_frame.set_index(\"id\")\n",
    "        cleaned_text_split = data_frame[\"cleaned texts\"].str.split(expand = True)\n",
    "        numbered_columns = cleaned_text_split.columns.values\n",
    "        cleaned_text_split.reset_index(inplace = True)\n",
    "        tidy_format = pd.melt(cleaned_text_split, id_vars=['id'], value_vars=numbered_columns)\n",
    "        tidy_fomat_1=tidy_format.rename(columns={\"variable\": \"num\", \"value\":\"word\"})\n",
    "        tidy_fromat_na=tidy_fomat_1.dropna()\n",
    "        tidy_fromat_sorted=tidy_fromat_na.sort_values(by=['id','num'])\n",
    "        tidy_format=tidy_fromat_sorted.set_index(\"id\")\n",
    "        tidy_format_sent_merged= tidy_format.join(lexicon, on=\"word\")\n",
    "        tidy_format_sent_merged[\"polarity\"]=tidy_format_sent_merged[\"polarity\"].fillna(0)\n",
    "        data_frame[\"VADER Sentiment\"]=tidy_format_sent_merged.groupby(tidy_format_sent_merged.index).polarity.agg(\"sum\")\n",
    "    return data_frame\n",
    "\n",
    "# this will analyze a single stirng usinng google sentment analysis and return its magnitude adn score \n",
    "def google_analyze_sentiment_string(text):\n",
    "    os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = r\"C:\\Users\\buzga\\Desktop\\Coding Stuff\\Twitter Reaserch\\Data\\Google api\\digital-arcade-333701-1e9b79d00038.json\"\n",
    "    client = language_v1.LanguageServiceClient()\n",
    "    type_ = language_v1.Document.Type.PLAIN_TEXT\n",
    "    language = \"en\"\n",
    "    document = {\"content\": text, \"type_\": type_, \"language\": language}\n",
    "    encoding_type = language_v1.EncodingType.UTF8\n",
    "    response = client.analyze_sentiment(request = {'document': document, 'encoding_type': encoding_type})\n",
    "    return response.document_sentiment.magnitude,response.document_sentiment.score\n",
    "\n",
    "# this will analyze a list of strings using google. \n",
    "def google_analyze_sentiment(DataFrame):\n",
    "    sentiment=[]\n",
    "    magnitude=[]\n",
    "    if(\"cleaned texts\" in DataFrame.columns):\n",
    "        for text in DataFrame[\"cleaned texts\"]:\n",
    "            magnitude.append(google_analyze_sentiment_string(text)[0])\n",
    "            sentiment.append(google_analyze_sentiment_string(text)[1])\n",
    "        DataFrame[\"Google Sentiment\"]=sentiment\n",
    "        DataFrame[\"Google Magnitude\"]=magnitude\n",
    "    return DataFrame\n",
    "# assuming our file system is set up a certain way gets us the name and path for quick cast \n",
    "def macros(root_path):\n",
    "    paths=[]\n",
    "    names=[]\n",
    "    for files in os.listdir(root_path): \n",
    "        for file in os.listdir(root_path+\"\\\\\"+files):   \n",
    "            names.append(files)\n",
    "            paths.append(root_path+\"\\\\\"+files+\"\\\\\"+file)\n",
    "    return paths,names\n",
    "\n",
    "\n",
    "def standardize(data, bad_cols):\n",
    "    # data either in the form of a path to a data frame or data frame. \n",
    "    # as well as a list of non-numeric cols. \n",
    "    # returns a data frame with all cols standardized. \n",
    "    data=pd.read_csv(data)\n",
    "    if(type(data)==str):\n",
    "        data=pd.read_csv(data)\n",
    "    col_names=list(data)\n",
    "    for i in bad_cols:\n",
    "        col_names.remove(i)\n",
    "    for i in col_names:\n",
    "        data[i+\" Standardized\"]=(data[i]-np.mean(data[i]))/np.std(data[i])\n",
    "    return data\n",
    "def single_day(df,date):\n",
    "    #takes data frame with date col and date, and returns the specfied day\n",
    "    return df.loc[df['Date'] == date]\n",
    "\n",
    "def date_in_range(df,start,stop):\n",
    "    # takes a data frame with a date coloum and finds index's between the two values inclusive\n",
    "    # where begin and stop are strings. \n",
    "    begin=df.index[df['Date'] == start].tolist()\n",
    "    end=df.index[df['Date'] == stop].tolist()\n",
    "    return df[begin[0]:end[0]+1]\n",
    "\n",
    "            \n",
    "# this will save our data to the computer so that we have it. \n",
    "def save_to_csv(data_frame,destination_path):\n",
    "    data_frame.to_csv(destination_path, mode='a', header=False)\n",
    "\n",
    "\n",
    "## this will sort the data frame by date. \n",
    "def sort_by_date(data):\n",
    "    temp=[]\n",
    "    data[\"created at\"][1][0:10]\n",
    "    for i in data[\"created at\"]:\n",
    "        temp.append(datetime.strptime(i[0:10],'%Y-%m-%d'))\n",
    "    data[\"date\"]=temp\n",
    "    data=data.sort_values(by=[\"date\"])\n",
    "    #data=data.set_index(\"date\")\n",
    "    return data \n",
    "#this will sort by year month date, and then sum over the sentiment coloumns \n",
    "def sumzarize(data):\n",
    "    df=sort_by_date(data)\n",
    "    df=df.drop(['id', 'User Name', 'orignal texts', 'created at', 'retweet count',\n",
    "         'reply count', 'like count', 'quote count', 'cleaned texts'], axis=1)\n",
    "    df=df.groupby(pd.Grouper(key='date', axis=0, \n",
    "                    freq='D')).sum()\n",
    "    return df\n",
    "\n",
    "def sum_row(path,bad_cols):\n",
    "    ## adds a row called sum that is the sum of all other numeric rows. \n",
    "    ## path string, path to data fraem \n",
    "    ## bad_cols list of non-numeric col names \n",
    "    data=pd.read_csv(path)\n",
    "    col_names=list(data)\n",
    "    for i in bad_cols:\n",
    "        col_names.remove(i)\n",
    "    data[\"Sum\"] = data[col_names].sum(axis=1) \n",
    "    return data\n",
    "\n",
    "\n",
    "def merge_on_date(left,right):\n",
    "    # does an inner join on the date col for two data frames. \n",
    "    return left.merge(right,on=\"Date\", how=\"inner\")\n",
    "\n",
    "def Summarized_CSV(data):\n",
    "    ## this takes in data, and saves a csv with summarized sentiment by day. \n",
    "    sumzarize(data).to_csv(r'C:\\Users\\buzga\\Desktop\\Coding Stuff\\Twitter Reaserch\\Data\\Destination\\Sumarized_sentiment.csv')\n",
    "\n",
    "\n",
    "# this will all the methods that are nesscary to analyze the sentiment in a single json file\n",
    "def quick_cast(path,name):\n",
    "    pattern_path=r\"C:\\Users\\buzga\\Desktop\\Coding Stuff\\Twitter Reaserch\\Data\\Pattern\\SnP500.csv\"\n",
    "    lexicon_path=r\"C:\\Users\\buzga\\Desktop\\Coding Stuff\\Twitter Reaserch\\Data\\vaderSentiment-master\\vaderSentiment-master\\vaderSentiment\\vader_lexicon.txt\"\n",
    "    destination_path=r\"C:\\Users\\buzga\\Desktop\\Coding Stuff\\Twitter Reaserch\\Data\\Destination\\aggregate_tweets.csv\"\n",
    "    data=get_data_frame(get_file(path), name)\n",
    "    pattern=get_pattern(pattern_path)+\"|\"+get_terms()\n",
    "    data=scrape(pattern,data)\n",
    "    lexicon=get_lexicon(lexicon_path) \n",
    "    data=google_analyze_sentiment(data)\n",
    "    data=Lexicon_analyze_sentiment(data, lexicon)\n",
    "    save_to_csv(data, destination_path)\n",
    "\n",
    "# just an over overloaded quick cast to avoid needed reperative function calls.\n",
    "def overload_quick_cast(path,name,lexicon,pattern,destination_path ):\n",
    "    data=get_data_frame(get_file(path), name)\n",
    "    data=scrape(pattern,data)\n",
    "    data=google_analyze_sentiment(data)\n",
    "    data=Lexicon_analyze_sentiment(data, lexicon)\n",
    "    save_to_csv(data, destination_path)\n",
    "\n",
    "\n",
    "# this takes a list of files and a lists of names and just runs quick cast mulitple times.\n",
    "def multi_cast(paths, names):\n",
    "    pattern_path=r\"C:\\Users\\buzga\\Desktop\\Coding Stuff\\Twitter Reaserch\\Data\\Pattern\\SnP500.csv\"\n",
    "    lexicon_path=r\"C:\\Users\\buzga\\Desktop\\Coding Stuff\\Twitter Reaserch\\Data\\vaderSentiment-master\\vaderSentiment-master\\vaderSentiment\\vader_lexicon.txt\"\n",
    "    destination_path=r\"C:\\Users\\buzga\\Desktop\\Coding Stuff\\Twitter Reaserch\\Data\\Destination\\aggregate_tweets.csv\"\n",
    "    pattern=get_pattern(pattern_path)+\"|\"+get_terms()\n",
    "    lexicon=get_lexicon(lexicon_path) \n",
    "    for i in range(len(paths)):\n",
    "        #print(i) # this is just a print traceback in case something is not running correctly. \n",
    "        overload_quick_cast(paths[i],names[i],lexicon,pattern,destination_path)\n",
    "# runs multi path given just a root path. \n",
    "def mega_cast(path):\n",
    "    paths, names= macros(path)\n",
    "    multi_cast(paths, names)\n",
    "\n",
    "def quick_read(path,name):\n",
    "    col_list = [\"Date\", \"Close\"]\n",
    "    df=pd.read_csv(path, usecols=col_list)\n",
    "    return df.rename(columns={'Close': name+' Close'})\n",
    "\n",
    "### some new helper functins \n",
    "def Stanadrd_graph(data,y,y_name, title):\n",
    "    ## makes standard line graph \n",
    "    ## df is a data frame that must have a date col \n",
    "    ## y is new line compared to fundementals \n",
    "    ## y_name is the name of new line \n",
    "    ## title is tile of graph\n",
    "    #'Final all Sentiment sum plus Final Standardized sum'\n",
    "    import matplotlib.pyplot as plt\n",
    "    X=data[\"Date\"]\n",
    "    Y1=data['Standardized Macro Indicators']\n",
    "    Y2=data[y]\n",
    "    Y3=data['Standardized Sum']\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xticks([0,len(data[\"Date\"])/2, len(data[\"Date\"])])\n",
    "    plt.plot(X, Y1, color=\"blue\", label = \"Observed Standardized Movements\")\n",
    "    plt.plot(X, Y2, color=\"red\", label = y_name)\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Deviations from mean\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    mse = mean_squared_error(Y1, Y2)\n",
    "    print(mse) \n",
    "    mse_precent=(mse-mean_squared_error(Y1, Y3))/mean_squared_error(Y1, Y3)\n",
    "    print(mse_precent*100)\n",
    " \n",
    "\n",
    "def implicit_standardized_sum(df, col_names, new_name):\n",
    "    #takes data frame and a list of colomn names sums and standrdizes those coloumsn \n",
    "    df[\"temp\"]=df[col_names].sum(axis=1) \n",
    "    df[new_name]=(df[\"temp\"]-np.mean(df[\"temp\"]))/np.std(df[\"temp\"])\n",
    "    df=df.drop('temp',axis=1)\n",
    "    return df\n",
    "def first_standard_graph(data, period):\n",
    "    import matplotlib.pyplot as plt\n",
    "    X=data[\"Date\"]\n",
    "    Y1=data['Standardized Macro Indicators']\n",
    "    #Y2=[np.mean(data['Fundementals Final Standrdized Sum'])]*len(data['Fundementals Final Standrdized Sum'])\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xticks([0,22, 42,63])\n",
    "    plt.plot(X, Y1, color=\"blue\", label = \"Observed Standardized Movements in Index\")\n",
    "    #plt.plot(X, Y2, color=\"red\", label = \"mean\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Deviations from mean\")\n",
    "    plt.title(\"Overall Movement in Macroeconomic Indicators Over Period \"+period)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "#mse = mean_squared_error(Y1, Y2)\n",
    "#print(mse)\n",
    "def period_analysis(data_prediod_1, period, start, end):\n",
    "    \n",
    "    data_prediod_1=date_in_range(data_prediod_1,start,end)\n",
    "    data_prediod_1= implicit_standardized_sum(data_prediod_1, ['Standardized Google Sentiment','Standardized Sum' ], \"d1\")\n",
    "    data_prediod_1= implicit_standardized_sum(data_prediod_1, ['Standardized Google Magnitude','Standardized Sum' ], \"d2\")\n",
    "    data_prediod_1= implicit_standardized_sum(data_prediod_1, ['Standardized VADER Sentiment','Standardized Sum' ], \"d3\")\n",
    "    data_prediod_1= implicit_standardized_sum(data_prediod_1, ['Standardized Google Sentiment','Standardized Google Magnitude' ], \"t1\")\n",
    "    data_prediod_1= implicit_standardized_sum(data_prediod_1, ['t1','Standardized Sum' ], \"d4\")\n",
    "    data_prediod_1= implicit_standardized_sum(data_prediod_1, ['Standardized Google Sentiment','Standardized VADER Sentiment' ], \"t2\")\n",
    "    data_prediod_1= implicit_standardized_sum(data_prediod_1, ['t2','Standardized Sum' ], \"d5\")\n",
    "    data_prediod_1= implicit_standardized_sum(data_prediod_1, ['Standardized Google Magnitude','Standardized VADER Sentiment' ], \"t3\")\n",
    "    data_prediod_1= implicit_standardized_sum(data_prediod_1, ['t3','Standardized Sum' ], \"d6\")\n",
    "    data_prediod_1= implicit_standardized_sum(data_prediod_1, ['Standardized Google Sentiment','Standardized Google Magnitude','Standardized VADER Sentiment' ], \"t4\")\n",
    "    data_prediod_1= implicit_standardized_sum(data_prediod_1, ['t4','Standardized Sum' ], \"d7\")\n",
    "    data_prediod_1= implicit_standardized_sum(data_prediod_1, ['Standardized Fundementals Sum','Standardized Psychology Sum' ], \"d8\")\n",
    "\n",
    "    first_standard_graph(data_prediod_1, period)\n",
    "    # ubtutak bkiinverg graogs \n",
    "\n",
    "    Stanadrd_graph(data_prediod_1,\"Standardized Fundementals Sum\",\"REH Fundamentals\", \"REH Fundamentals Compared to Index Movement Period \"+period)\n",
    "    Stanadrd_graph(data_prediod_1,\"d8\",\"Behavioral Psychology\", \"Behavioral Psychology Compared to Index Movement Period \"+period)\n",
    "    Stanadrd_graph(data_prediod_1,\"Standardized Sum\",\"KU Model\", \"Knightian Uncertainty Compared to Index Movement Period \"+period)\n",
    "    ## inittal tiwtter grahs \n",
    "    Stanadrd_graph(data_prediod_1,\"Standardized Google Sentiment\",\"Google Sentiment\", \"Google Sentiment Compared to Index Movement Period \"+period)\n",
    "    Stanadrd_graph(data_prediod_1,\"Standardized Google Magnitude\",\"Google Magnitude\", \"Google Magnitude Compared to Index Movement Period \"+period)\n",
    "    Stanadrd_graph(data_prediod_1,\"Standardized VADER Sentiment\",\"VADER Sentiment\", \"VADER Sentiment Compared to Index Movement Period \"+period)\n",
    "\n",
    "    ### overall \n",
    "    Stanadrd_graph(data_prediod_1,\"d1\",\"Bloomberg + Google Sentiment\", \"Google Sentiment and Bloomberg Compared to Index Movement Period \"+period)\n",
    "    Stanadrd_graph(data_prediod_1,\"d2\",\"Bloomberg + Google Magnitude\", \"Google Magnitude and Bloomberg Compared to Index Movement Period \"+period)\n",
    "    Stanadrd_graph(data_prediod_1,\"d3\",\"Bloomberg + VADER Sentiment\", \"Google VADER Sentiment and Bloomberg Compared to Index Movement Period \"+period)\n",
    "    Stanadrd_graph(data_prediod_1,\"d4\",\"Google Sentiment + Magnitude + Bloomberg\", \"Google Sentiment and Magnitude Compared to Index Movement Period \"+period)\n",
    "    Stanadrd_graph(data_prediod_1,\"d5\",\"Google Sentiment + VADER + Bloomberg\" , \"Google Sentiment and VADER Compared to Index Movement Period \"+period)\n",
    "    Stanadrd_graph(data_prediod_1,\"d6\",\"Google Magnitude + VADER + Bloomberg\", \"Google Magnitude and VADER Compared to Index Movement Period \"+period)\n",
    "    Stanadrd_graph(data_prediod_1,\"d7\",\"Whole Twitter + Bloomberg\", \"Whole Twitter Sentiment and Bloomberg Compared to Index Movement Period \"+period)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
